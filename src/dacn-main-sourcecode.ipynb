{"cells":[{"cell_type":"markdown","metadata":{},"source":["# ĐỒ ÁN CHUYÊN NGÀNH"]},{"cell_type":"markdown","metadata":{},"source":["## Cài đặt thư viện"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-07-10T13:11:55.863109Z","iopub.status.busy":"2025-07-10T13:11:55.862841Z","iopub.status.idle":"2025-07-10T13:13:22.851103Z","shell.execute_reply":"2025-07-10T13:13:22.850203Z","shell.execute_reply.started":"2025-07-10T13:11:55.863089Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\n","Collecting fpdf\n","  Downloading fpdf-1.7.2.tar.gz (39 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n","Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.14)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: fpdf\n","  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=cce94263ab4bb546324fc5c71708522243721a80b0f3c362ed5fc0f5a43b9a9d\n","  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n","Successfully built fpdf\n","Installing collected packages: fpdf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, faiss-cpu, bitsandbytes\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n","    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.10.19\n","    Uninstalling nvidia-curand-cu12-10.3.10.19:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n","    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n","      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n","    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n","      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n","    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n","    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n","Successfully installed bitsandbytes-0.46.1 faiss-cpu-1.11.0 fpdf-1.7.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}],"source":["!pip install transformers torch bitsandbytes accelerate faiss-cpu ipywidgets fpdf"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:13:22.852864Z","iopub.status.busy":"2025-07-10T13:13:22.852626Z","iopub.status.idle":"2025-07-10T13:13:50.994284Z","shell.execute_reply":"2025-07-10T13:13:50.993346Z","shell.execute_reply.started":"2025-07-10T13:13:22.852841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Collecting chromadb\n","  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Collecting gradio\n","  Downloading gradio-5.36.2-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Collecting build>=1.0.3 (from chromadb)\n","  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.4)\n","Collecting pybase64>=1.4.1 (from chromadb)\n","  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n","Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n","  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.0rc1)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.40.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n","Collecting brotli>=1.1.0 (from gradio)\n","  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio)\n","  Downloading fastapi-0.116.0-py3-none-any.whl.metadata (28 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.10.4 (from gradio)\n","  Downloading gradio_client-1.10.4-py3-none-any.whl.metadata (7.1 kB)\n","Collecting groovy~=0.1 (from gradio)\n","  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n","Collecting python-multipart>=0.0.18 (from gradio)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Collecting ruff>=0.9.3 (from gradio)\n","  Downloading ruff-0.12.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n","  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio)\n","  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)\n","Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n","  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.4->gradio) (2025.3.2)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio)\n","  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.1.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.1.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n","Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n","INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.34.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.34.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.34.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\n","INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n","Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.1.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.3.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n","Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading gradio-5.36.2-py3-none-any.whl (59.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.10.4-py3-none-any.whl (323 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n","Downloading fastapi-0.116.0-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n","Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n","Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading ruff-0.12.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n","Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n","Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n","Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n","Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=06352f0ae571a4b3d46b942f6f6629d44acec322e28210ba0fb85726986aac86\n","  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n","Successfully built pypika\n","Installing collected packages: pypika, durationpy, brotli, uvloop, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, groovy, ffmpy, bcrypt, backoff, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, safehttpx, opentelemetry-semantic-conventions, kubernetes, gradio-client, fastapi, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, gradio, chromadb\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 8.7.0\n","    Uninstalling importlib_metadata-8.7.0:\n","      Successfully uninstalled importlib_metadata-8.7.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.31.1\n","    Uninstalling opentelemetry-api-1.31.1:\n","      Successfully uninstalled opentelemetry-api-1.31.1\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n","    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.31.1\n","    Uninstalling opentelemetry-sdk-1.31.1:\n","      Successfully uninstalled opentelemetry-sdk-1.31.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 brotli-1.1.0 build-1.2.2.post1 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.116.0 ffmpy-0.6.0 gradio-5.36.2 gradio-client-1.10.4 groovy-0.1.2 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 posthog-5.4.0 pybase64-1.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.1 python-multipart-0.0.20 ruff-0.12.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.3 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install sentence-transformers chromadb google-generativeai gradio"]},{"cell_type":"markdown","metadata":{},"source":["## Import thư viện"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:13:50.995821Z","iopub.status.busy":"2025-07-10T13:13:50.995509Z","iopub.status.idle":"2025-07-10T13:14:17.229541Z","shell.execute_reply":"2025-07-10T13:14:17.228767Z","shell.execute_reply.started":"2025-07-10T13:13:50.995785Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-07-10 13:14:02.275317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1752153242.461171      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1752153242.524777      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import json\n","import torch\n","import faiss\n","import re\n","import os\n","import requests\n","import numpy as np\n","from datetime import datetime\n","from typing import List, Dict, Tuple\n","import ipywidgets as widgets\n","from collections import defaultdict\n","from IPython.display import display, clear_output, FileLink\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n","from sentence_transformers import SentenceTransformer\n","from huggingface_hub import login\n","import google.generativeai as genai\n","\n","# Đăng nhập Huggingface\n","login(\"xxx\")\n","\n","# Cấu hình cache -> tăng tốc độ tải mô hình\n","os.environ['HF_HOME'] = '/kaggle/working/huggingface_cache'"]},{"cell_type":"markdown","metadata":{},"source":["## IOC Extractors"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:14:17.231843Z","iopub.status.busy":"2025-07-10T13:14:17.231324Z","iopub.status.idle":"2025-07-10T13:14:36.502579Z","shell.execute_reply":"2025-07-10T13:14:36.501571Z","shell.execute_reply.started":"2025-07-10T13:14:17.231821Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting iocsearcher\n","  Downloading iocsearcher-2.5.12-py3-none-any.whl.metadata (11 kB)\n","Collecting base58 (from iocsearcher)\n","  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from iocsearcher) (4.13.3)\n","Collecting bech32 (from iocsearcher)\n","  Downloading bech32-1.2.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting cashaddress (from iocsearcher)\n","  Downloading cashaddress-1.0.6.tar.gz (5.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting cbor (from iocsearcher)\n","  Downloading cbor-1.0.0.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting docx2python (from iocsearcher)\n","  Downloading docx2python-3.5.0-py3-none-any.whl.metadata (18 kB)\n","Collecting eth-hash[pycryptodome] (from iocsearcher)\n","  Downloading eth_hash-0.7.1-py3-none-any.whl.metadata (4.2 kB)\n","Collecting intervaltree (from iocsearcher)\n","  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting langdetect (from iocsearcher)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from iocsearcher) (5.3.1)\n","Collecting pdfminer.six (from iocsearcher)\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Collecting phonenumbers (from iocsearcher)\n","  Downloading phonenumbers-9.0.9-py2.py3-none-any.whl.metadata (11 kB)\n","Collecting python-magic (from iocsearcher)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Collecting readabilipy (from iocsearcher)\n","  Downloading readabilipy-0.3.0-py3-none-any.whl.metadata (8.9 kB)\n","Collecting solders (from iocsearcher)\n","  Downloading solders-0.26.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->iocsearcher) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->iocsearcher) (4.13.2)\n","Collecting paragraphs (from docx2python->iocsearcher)\n","  Downloading paragraphs-1.0.1-py3-none-any.whl.metadata (5.5 kB)\n","Collecting types-lxml (from docx2python->iocsearcher)\n","  Downloading types_lxml-2025.3.30-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pycryptodome<4,>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from eth-hash[pycryptodome]->iocsearcher) (3.22.0)\n","Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->iocsearcher) (2.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect->iocsearcher) (1.17.0)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->iocsearcher) (3.4.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->iocsearcher) (44.0.3)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from readabilipy->iocsearcher) (1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from readabilipy->iocsearcher) (2024.11.6)\n","Collecting jsonalias==0.1.1 (from solders->iocsearcher)\n","  Downloading jsonalias-0.1.1-py3-none-any.whl.metadata (469 bytes)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->iocsearcher) (1.17.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->readabilipy->iocsearcher) (0.5.1)\n","Collecting cssselect~=1.2 (from types-lxml->docx2python->iocsearcher)\n","  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting types-html5lib~=1.1.11.20241018 (from types-lxml->docx2python->iocsearcher)\n","  Downloading types_html5lib-1.1.11.20250708-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->iocsearcher) (2.22)\n","Downloading iocsearcher-2.5.12-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Downloading bech32-1.2.0-py3-none-any.whl (4.6 kB)\n","Downloading docx2python-3.5.0-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading phonenumbers-9.0.9-py2.py3-none-any.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading readabilipy-0.3.0-py3-none-any.whl (22 kB)\n","Downloading solders-0.26.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading jsonalias-0.1.1-py3-none-any.whl (1.3 kB)\n","Downloading eth_hash-0.7.1-py3-none-any.whl (8.0 kB)\n","Downloading paragraphs-1.0.1-py3-none-any.whl (5.1 kB)\n","Downloading types_lxml-2025.3.30-py3-none-any.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n","Downloading types_html5lib-1.1.11.20250708-py3-none-any.whl (22 kB)\n","Building wheels for collected packages: cashaddress, cbor, intervaltree, langdetect\n","  Building wheel for cashaddress (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cashaddress: filename=cashaddress-1.0.6-py3-none-any.whl size=6434 sha256=35528872a7921d5f5e65d86ef6dd89692e7034350dc329dc40caa3ce9226bb0f\n","  Stored in directory: /root/.cache/pip/wheels/ab/4e/08/a6f380a0bb2a7c4e0654e50e2eb9a3358cc58d159a68e004e1\n","  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53930 sha256=e569b946593fbcf9fa1e1d06b708a1ed8b01e5a215d39b6b7ac9436b4b2d5d9b\n","  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n","  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=e058ac553ba370bb8aa65d06daf856a68fa9357b8afe0a9ee23b80b4b7516703\n","  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=204693518dbafc6e9037acf969afda1aa66bca38a2c68738f4fb329ca99f37bb\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built cashaddress cbor intervaltree langdetect\n","Installing collected packages: phonenumbers, cbor, types-html5lib, python-magic, paragraphs, langdetect, jsonalias, intervaltree, eth-hash, cssselect, cashaddress, bech32, base58, types-lxml, solders, readabilipy, pdfminer.six, docx2python, iocsearcher\n","Successfully installed base58-2.1.1 bech32-1.2.0 cashaddress-1.0.6 cbor-1.0.0 cssselect-1.3.0 docx2python-3.5.0 eth-hash-0.7.1 intervaltree-3.1.0 iocsearcher-2.5.12 jsonalias-0.1.1 langdetect-1.0.9 paragraphs-1.0.1 pdfminer.six-20250506 phonenumbers-9.0.9 python-magic-0.4.27 readabilipy-0.3.0 solders-0.26.0 types-html5lib-1.1.11.20250708 types-lxml-2025.3.30\n"]}],"source":["!pip install iocsearcher"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:14:39.848718Z","iopub.status.busy":"2025-07-10T13:14:39.848514Z","iopub.status.idle":"2025-07-10T13:14:39.864949Z","shell.execute_reply":"2025-07-10T13:14:39.864154Z","shell.execute_reply.started":"2025-07-10T13:14:39.848695Z"},"trusted":true},"outputs":[],"source":["# Search url, hash, domain, ip\n","\n","from pprint import pprint\n","import requests\n","from base64 import urlsafe_b64encode\n","\n","def search_threatfox(ioc):\n","    url = \"https://threatfox-api.abuse.ch/api/v1/\"\n","    headers = {\n","        \"Auth-Key\": \"xxx\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    data = {\n","        \"query\": \"search_ioc\",\n","        \"search_term\": ioc,\n","        \"exact_match\": True\n","    }\n","\n","    response = requests.post(url, json=data, headers=headers)\n","\n","    if response.ok:\n","        res = response.json()\n","        if res['query_status'] == 'ok':\n","            result = []\n","            for i in res['data']:\n","                result_ioc = {}\n","                result_ioc['ioc_desc'] = i['ioc_type_desc']\n","                result_ioc['malware'] = i['malware']\n","                result_ioc['malware_alias'] = i['malware_alias']\n","                result_ioc['threat_type'] = i['threat_type']\n","                result_ioc['threat_type_desc'] = i['threat_type_desc']\n","            return result\n","        return None\n","    else:\n","        print(f\"[!] HTTP {response.status_code} - {response.reason}\")\n","        return None\n","\n","\n","\n","def search_hash(hash):\n","    # URL của API\n","    url = \"https://hybrid-analysis.com/api/v2/search/hash\"\n","    \n","    headers = {\n","        \"accept\": \"application/json\",\n","        \"api-key\": \"xxx\",\n","        \"Content-Type\": \"application/x-www-form-urlencoded\",\n","    }\n","\n","    data = {\n","        \"hash\": hash,\n","    }\n","    response = requests.post(url, headers=headers, data=data)\n","    if response.status_code == 200:\n","        result = response.json()\n","        filtered_result = []\n","        for i in range(len(result)):\n","          if result[i]['signatures'] != []:\n","            for j in range(len(result[i]['signatures'])):\n","                if result[i]['signatures'][j]['threat_level'] != 0 or result[i]['signatures'][j]['attck_id'] != None:\n","                    sigdict = {}\n","                    sigdict['name'] = result[i]['signatures'][j]['name']\n","                    sigdict['description'] = result[i]['signatures'][j]['description']\n","                    sigdict['threat_level'] = result[i]['signatures'][j]['threat_level']\n","                    if result[i]['signatures'][j]['attck_id'] != None:\n","                        sigdict['ttp'] = result[i]['signatures'][j]['attck_id']\n","                    filtered_result.append(sigdict)\n","        return filtered_result if filtered_result else None\n","    else:\n","        print(f\"Request failed with status code {response.status_code}\")\n","        return None"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:14:39.865955Z","iopub.status.busy":"2025-07-10T13:14:39.865721Z","iopub.status.idle":"2025-07-10T13:14:39.959576Z","shell.execute_reply":"2025-07-10T13:14:39.959032Z","shell.execute_reply.started":"2025-07-10T13:14:39.865931Z"},"trusted":true},"outputs":[],"source":["import iocsearcher\n","from iocsearcher.searcher import Searcher\n","from pprint import pprint\n","import json\n","\n","# with open(\"/kaggle/input/dynamic-analysis/report-mod.json\") as f:\n","#     raw_dynamic = f.read()\n","# with open(\"/kaggle/input/static-analysis/21.json\") as f:\n","#     raw_static = f.read()\n","\n","def ioc_extractors(raw_dynamic, raw_static):\n","    print(\"IOC Extractors\")\n","    raw_dynamic = json.dumps(raw_dynamic)\n","    raw_static = json.dumps(raw_static)\n","    ioc = []\n","    searcher = Searcher()\n","    # Extract ioc trong dynamic analysis và search Threat intelligence\n","    dynamic_ioc = searcher.search_data(raw_dynamic)\n","    for i in dynamic_ioc.copy():\n","        if i.name not in [\"url\", \"fqdn\", \"sha1\", \"ip4\", \"ip6\", \"md5\", \"sha1\", \"sha256\"]:\n","            dynamic_ioc.remove(i)\n","    \n","    for i in dynamic_ioc:\n","        if i.name == \"ip4\" or i.name == \"ip6\" or i.name == \"fqdn\" or i.name == \"url\":\n","            if search_threatfox(i.value) != None:\n","                tmp = {}\n","                tmp['type'] = i.name\n","                tmp['ioc'] = i.value\n","                tmp['info'] = search_threatfox(i.value)\n","                ioc.append(tmp)\n","# Extract ioc trong static analysis và search Threat intelligence\n","    static_ioc = searcher.search_data(raw_static)\n","    \n","    for i in static_ioc.copy():\n","        if i.name not in [\"url\", \"fqdn\", \"sha1\", \"ip4\", \"ip6\", \"md5\", \"sha1\", \"sha256\"]:\n","            static_ioc.remove(i)\n","    for i in static_ioc:\n","        if i.name == \"ip4\" or i.name == \"ip6\" or i.name == \"fqdn\" or i.name == \"url\":\n","            if search_threatfox(i.value) != None:\n","                tmp = {}\n","                tmp['type'] = i.name\n","                tmp['ioc'] = i.value\n","                tmp['info'] = search_threatfox(i.value)\n","                ioc.append(tmp)\n","\n","\n","    return ioc\n"]},{"cell_type":"markdown","metadata":{},"source":["## Cấu hình mô hình BAAI (Embedded model)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:14:39.960335Z","iopub.status.busy":"2025-07-10T13:14:39.960154Z","iopub.status.idle":"2025-07-10T13:14:47.823710Z","shell.execute_reply":"2025-07-10T13:14:47.823099Z","shell.execute_reply.started":"2025-07-10T13:14:39.960319Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"123cf97d11854920a60ffec6d98ae3a1","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8ba2f80bdfc4dc5b8132d66e13d1619","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cf213b1eaee4256882c3e0b07df7149","version_major":2,"version_minor":0},"text/plain":["README.md: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f063844ac50459db68a8619acef2e84","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b12eb4b1701e4cff840bf5b01f65b3f0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d6c3dca8b0745b3988aa14d6ebb8ec5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e3e1304c2b54f38bf4e08662519c604","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18820cc0176146dfbbc3ca27a65a8e39","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e97843e49b5450eb72e153704ff63cb","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8793a70a3a5740afb1d7543fc83713ae","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f48460425824c5f91d41d56e7e16f3b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["SentenceTransformer(\n","  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n","  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n","  (2): Normalize()\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Mô hình BAAI\n","embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n","embedding_model = SentenceTransformer(embedding_model_name)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","embedding_model = embedding_model.to(device)\n","embedding_model.eval()"]},{"cell_type":"markdown","metadata":{},"source":["##  Create RAG ChromaDB"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-06-13T09:13:32.951477Z","iopub.status.busy":"2025-06-13T09:13:32.950878Z","iopub.status.idle":"2025-06-13T10:23:04.587396Z","shell.execute_reply":"2025-06-13T10:23:04.586300Z","shell.execute_reply.started":"2025-06-13T09:13:32.951454Z"},"trusted":true},"outputs":[{"ename":"AttributeError","evalue":"'Client' object has no attribute 'persist'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_175/2628799329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Lưu lại DB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inserted {len(ids_all)} behavior embeddings into Chroma.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Client' object has no attribute 'persist'"]}],"source":["import os\n","import json\n","import torch\n","from sentence_transformers import SentenceTransformer\n","import chromadb\n","\n","# Load behavior data\n","with open(\"/kaggle/input/behaviors-ttp/behaviors_ttp.json\", \"r\", encoding=\"utf-8\") as f:\n","    behaviors = json.load(f)\n","\n","# ChromaDB setup\n","PERSIST_DIR = \"/kaggle/working/chroma_db\"\n","client = chromadb.PersistentClient(path=PERSIST_DIR)\n","collection = client.get_or_create_collection(\n","    name=\"ttp_behaviors\",\n","    metadata={\"description\": \"Behavior text + TTP mapping\"}\n",")\n","\n","# Utility: chia batch nhỏ để tránh lỗi giới hạn\n","def batched(iterable, batch_size):\n","    for i in range(0, len(iterable), batch_size):\n","        yield iterable[i:i + batch_size]\n","\n","# Cấu hình batch encode và batch add\n","encode_batch_size = 16\n","insert_batch_size = 1000\n","\n","# Encode theo batch\n","ids_all, docs_all, metas_all, embs_all = [], [], [], []\n","\n","for i in range(0, len(behaviors), encode_batch_size):\n","    batch = behaviors[i : i + encode_batch_size]\n","    texts = [b[\"behavior\"] for b in batch]\n","    embeddings = embedding_model.encode(\n","        texts,\n","        batch_size=encode_batch_size,\n","        convert_to_tensor=True,\n","        show_progress_bar=False\n","    ).cpu().tolist()\n","\n","    for idx, (b, emb) in enumerate(zip(batch, embeddings)):\n","        ids_all.append(f\"behavior-{i+idx}\")\n","        docs_all.append(b[\"behavior\"])\n","        metas_all.append({\"ttp\": b[\"ttp\"]})\n","        embs_all.append(emb)\n","\n","# Add từng chunk nhỏ vào ChromaDB\n","for id_chunk, doc_chunk, meta_chunk, emb_chunk in zip(\n","    batched(ids_all, insert_batch_size),\n","    batched(docs_all, insert_batch_size),\n","    batched(metas_all, insert_batch_size),\n","    batched(embs_all, insert_batch_size)\n","):\n","    collection.add(\n","        ids=id_chunk,\n","        embeddings=emb_chunk,\n","        metadatas=meta_chunk,\n","        documents=doc_chunk\n","    )\n","\n","\n","print(f\"Inserted {len(ids_all)} behavior embeddings into Chroma.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Gemini-2.0-flash"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:14:47.824711Z","iopub.status.busy":"2025-07-10T13:14:47.824450Z","iopub.status.idle":"2025-07-10T13:14:47.829903Z","shell.execute_reply":"2025-07-10T13:14:47.829181Z","shell.execute_reply.started":"2025-07-10T13:14:47.824684Z"},"trusted":true},"outputs":[],"source":["\n","import google.generativeai as genai\n","import os\n","\n","def get_gemini_response(prompt_text: str) -> str:\n","    try:\n","\n","        genai.configure(api_key=\"xxx\")\n","\n","\n","        generation_config = {\n","            \"temperature\": 0.3,\n","            \"top_p\": 0.8,\n","            \"top_k\": 1,\n","            \"max_output_tokens\": 2048,\n","        }\n","\n","        # Chọn model để sử dụng\n","        model = genai.GenerativeModel(\n","            model_name=\"gemini-2.0-flash\",\n","            generation_config=generation_config\n","        )\n","\n","        response = model.generate_content(prompt_text)\n","        return response.text\n","    except Exception as e:\n","        return f\"Đã xảy ra lỗi: {e}\""]},{"cell_type":"markdown","metadata":{},"source":["##  Mistral-7B-Instruct-v0.3"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-07-06T09:18:55.223637Z","iopub.status.busy":"2025-07-06T09:18:55.223355Z","iopub.status.idle":"2025-07-06T09:20:10.960071Z","shell.execute_reply":"2025-07-06T09:20:10.959365Z","shell.execute_reply.started":"2025-07-06T09:18:55.223612Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c569d8e416f54d2b8060d26a96aacbe6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb0b88bc2e8f48258275a5cabfd490a6","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87df2dd2a16c4e26b7a3ea7867a35612","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc673de98f3d42f8b67e11fcf8d9a482","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9df64127787c4d868724b60fe0748caf","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7d8d0712b414728b6e6754c4f9c9b8f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4955a20a405644e7a9fb0a71c669abb8","version_major":2,"version_minor":0},"text/plain":["Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e64a5245c42f4512927caceee6c70353","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8914b8ec35594e438ded495379b378dc","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac3b1684d1a64d96a4670b0f297e292e","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52f2fc19e2474a5a8f660980d09d6660","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76f22bbcac3b4bdca7a108c6bc320786","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","generator_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","hf_token = \"xxx\"\n","\n","\n","generator_tokenizer = AutoTokenizer.from_pretrained(\n","    generator_model_name,\n","    use_auth_token=hf_token\n",")\n","generator_model = AutoModelForCausalLM.from_pretrained(\n","    generator_model_name,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    use_auth_token=hf_token\n",")\n","\n","generator_tokenizer.pad_token = generator_tokenizer.eos_token"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-07-06T09:21:39.287121Z","iopub.status.busy":"2025-07-06T09:21:39.286418Z","iopub.status.idle":"2025-07-06T09:21:39.292570Z","shell.execute_reply":"2025-07-06T09:21:39.291749Z","shell.execute_reply.started":"2025-07-06T09:21:39.287098Z"},"trusted":true},"outputs":[],"source":["def mistral_answer(prompt):\n","    \n","    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(generator_model.device)\n","\n","    with torch.no_grad():\n","        output = generator_model.generate(\n","            **inputs,\n","            max_new_tokens=2048,\n","            do_sample=False,\n","            repetition_penalty=1.1\n","        )\n","    input_len = inputs[\"input_ids\"].shape[1] \n","    generated_tokens = output[0][input_len:]\n","    full_answer = generator_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","    if \"Response:\" in full_answer:\n","        print(\"split\")\n","        answer = full_answer.split(\"Response:\", 1)[1].strip()\n","    else:\n","        answer = full_answer.strip()\n","    return answer"]},{"cell_type":"markdown","metadata":{},"source":["##  Meta-Llama-3.1-8B-Instruct"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:14:47.832502Z","iopub.status.busy":"2025-07-10T13:14:47.832217Z","iopub.status.idle":"2025-07-10T13:17:52.511721Z","shell.execute_reply":"2025-07-10T13:17:52.511163Z","shell.execute_reply.started":"2025-07-10T13:14:47.832481Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86fccdce6bdf46b99c834611d8e7fc2d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18d936ed17414623b65a9ab9c7744950","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a43f99660cd4928b9d749ac29ff8297","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83ab2513b37a4335bd26ccb66a4887de","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6b54b9368db4673ba23718f41621729","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"248c215c91f64294ae1fbd79d0152feb","version_major":2,"version_minor":0},"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85c5c1fa868b477286e8ad57e50e7213","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11d38dccb7f1426081e53d4c14b730e2","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc9d6670e3df48b7aacb49bf50eb9ffb","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"680a645c6c2c41fd8e15d120806b1d37","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"418dbb292c3a4672ad576ed8737596b5","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"028667d090d54214bf560aaa2a5af4d8","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","generator_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n","hf_token = \"xxx\"\n","\n","\n","generator_tokenizer = AutoTokenizer.from_pretrained(\n","    generator_model_name,\n","    use_auth_token=hf_token\n",")\n","generator_model = AutoModelForCausalLM.from_pretrained(\n","    generator_model_name,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    use_auth_token=hf_token\n",")\n","\n","generator_tokenizer.pad_token = generator_tokenizer.eos_token"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2025-07-06T03:00:48.284071Z","iopub.status.busy":"2025-07-06T03:00:48.283668Z","iopub.status.idle":"2025-07-06T03:00:48.292699Z","shell.execute_reply":"2025-07-06T03:00:48.291354Z","shell.execute_reply.started":"2025-07-06T03:00:48.284042Z"},"trusted":true},"outputs":[],"source":["def meta_llama_answer(prompt):\n","    \n","    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(generator_model.device)\n","\n","    with torch.no_grad():\n","        output = generator_model.generate(\n","            **inputs,\n","            max_new_tokens=2048,\n","            do_sample=False,\n","            repetition_penalty=1.1\n","        )\n","    input_len = inputs[\"input_ids\"].shape[1] \n","    generated_tokens = output[0][input_len:]\n","    full_answer = generator_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","\n","    if \"Response:\" in full_answer:\n","        print(\"split\")\n","        answer = full_answer.split(\"Response:\", 1)[1].strip()\n","    else:\n","        answer = full_answer.strip()\n","    \n","    return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(get_gemini_response(\"Reads information about supported languages \\\"21.exe\\\" (Path: \\\"HKLM\\\\SYSTEM\\\\CONTROLSET001\\\\CONTROL\\\\NLS\\\\CUSTOMLOCALE\\\"; Key: \\\"EMPTY\\\")\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Xử lý Dynamic Analysis"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:01.697337Z","iopub.status.busy":"2025-07-10T13:18:01.697035Z","iopub.status.idle":"2025-07-10T13:18:01.706117Z","shell.execute_reply":"2025-07-10T13:18:01.705414Z","shell.execute_reply.started":"2025-07-10T13:18:01.697316Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json\n","\n","# Hàm trích xuất câu trả lời từ LLM\n","def extract_balanced_json(text, after_keyword=\"Return only the output JSON object as described.\"):\n","    index = text.find(after_keyword)\n","    if index == -1:\n","        print('Key word not found')\n","        return None\n","    \n","    remaining = text[index + len(after_keyword):]\n","    start = remaining.find('{')\n","    if start == -1:\n","        print('Key word json not found')\n","        return None\n","\n","    brace_count = 0\n","    for i in range(start, len(remaining)):\n","        if remaining[i] == '{':\n","            brace_count += 1\n","        elif remaining[i] == '}':\n","            brace_count -= 1\n","            if brace_count == 0:\n","                json_str = remaining[start:i+1]\n","                try:\n","                    return json.loads(json_str)\n","                except json.JSONDecodeError:\n","                    return json_str.strip()\n","    return None\n","\n","def transform_malware_json(input_json: dict, tokenizer, model, device='cuda'):\n","    \n","    system_prompt = \"\"\"You are a JSON-to-Insight transformer for malware dynamic analysis. You only respond in JSON.\"\"\"\n","    user_prompt = f\"\"\"\n","    You are a JSON-to-Insight transformer for malware dynamic analysis. You only respond in JSON.\n","Given a malware analysis report in JSON format, extract and output a new JSON object with the following structure:\n","\n","{{\n","    \"behavior\": <string, taken directly from the \"description\" field>,\n","    \"evidence\": <string or list of strings, semantically summarized from the \"marks\" field without redundancy>\n","}}\n","\n","Instructions:\n","- Always extract the exact text from the \"description\" field and place it as the value of the \"behavior\" key.\n","- For \"evidence\":\n","    - Review all entries in the \"marks\" array.\n","    - Summarize each entry into a short semantic description.\n","    - If multiple entries are semantically similar, include only one.\n","    - If they are distinct, output them as a list of short meaningful evidence strings.\n","\n","Now convert this JSON:\n","\n","{json.dumps(input_json, indent=4)}\n","\n","Return only the output JSON object as described.\n","\n","Response: \n","\"\"\"\n","\n","    inputs = tokenizer(user_prompt, return_tensors=\"pt\", return_attention_mask=True, truncation=True).to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            max_new_tokens=1024,\n","            do_sample=False,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    try:\n","        return extract_balanced_json(output_text)\n","    except json.JSONDecodeError:\n","        return {\"error\": \"Model output is not valid JSON\", \"output\": model_response}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","input_data = \"\"\"\n","\n","\"\"\"\n","print(type(input_data))\n","result = transform_malware_json(input_data, generator_tokenizer, generator_model)\n","print(result)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:05.950356Z","iopub.status.busy":"2025-07-10T13:18:05.949901Z","iopub.status.idle":"2025-07-10T13:18:05.954589Z","shell.execute_reply":"2025-07-10T13:18:05.953900Z","shell.execute_reply.started":"2025-07-10T13:18:05.950332Z"},"trusted":true},"outputs":[],"source":["# with open(\"/kaggle/input/dynamic-analysis/report-mod.json\", \"r\") as f:\n","#   data = json.load(f)\n","\n","def extract_signatures(data):\n","    signatures = []\n","    cnt = 1\n","    for i in data['signatures']:\n","        tmp = transform_malware_json(i, generator_tokenizer, generator_model)\n","        if i[\"ttp\"] != {}:\n","            tmp[\"ttp\"] = list(i[\"ttp\"].keys())\n","        signatures.append(tmp)\n","    return signatures"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:11.862836Z","iopub.status.busy":"2025-07-10T13:18:11.862137Z","iopub.status.idle":"2025-07-10T13:18:11.873505Z","shell.execute_reply":"2025-07-10T13:18:11.872740Z","shell.execute_reply.started":"2025-07-10T13:18:11.862814Z"},"trusted":true},"outputs":[],"source":["def extract_behavior(data):\n","    behaviors = []\n","    # Trích xuất processes behavior\n","    for j in data[\"behavior\"][\"processes\"]:\n","      s = j[\"process_name\"] + \" (\" + j[\"process_path\"] + \") is excuted by command-line \" + j[\"command_line\"]\n","      behaviors.append(s)\n","      if j[\"modules\"] != []:  \n","        for k in j[\"modules\"]:\n","          s = j[\"process_name\"] + \" load module: \"\n","          s += k[\"basename\"] + \"(Filepath\" + k[\"filepath\"] +\")\"\n","          behaviors.append(s)\n","      if j[\"calls\"] != []:\n","        for k in j[\"calls\"]:\n","          s = j[\"process_name\"] + \" call: \" + k[\"api\"]\n","          if k[\"arguments\"]:\n","            s += \" with arguments: \"\n","            for key, value in k[\"arguments\"].items():\n","                if isinstance(value, str) and not value.isascii():\n","                    continue\n","                s += f'{key} = {value}, '\n","          behaviors.append(s)\n","    # Trích xuất generic behavior\n","    for x in range(len(data[\"behavior\"][\"generic\"])):\n","      if data[\"behavior\"][\"generic\"][x][\"summary\"] != {}:\n","        \n","        for key, value in data[\"behavior\"][\"generic\"][x][\"summary\"].items():\n","          s = data[\"behavior\"][\"generic\"][x][\"process_name\"] + \" (Filepath: \" + data[\"behavior\"][\"generic\"][x][\"process_path\"] + \") \"\n","          if key == \"file_created\":\n","            s += \"create files: \"\n","            for file in value:\n","              s += file + \", \"\n","            behaviors.append(s)\n","          if key == \"file_opened\":\n","            s += \"open files: \"\n","            for file in value:\n","              s += file + \", \"\n","            behaviors.append(s)\n","          if key == \"file_read\":\n","            s += \"read files: \"\n","            for file in value:\n","              s += file + \", \"\n","            behaviors.append(s)\n","          if key == \"file_written\":\n","            s += \"write files: \"\n","            for file in value:\n","              s += file + \", \"\n","            behaviors.append(s)\n","          if key == \"file_deleted\":\n","            s += \"delete files: \"\n","            for file in value:\n","              s += file + \", \"\n","            behaviors.append(s)\n","          if key == \"regkey_opened\":\n","            s += \"open registry keys: \"\n","            for key in value:\n","              s += key + \", \"\n","            behaviors.append(s)\n","          if key == \"regkey_read\":\n","            s += \"read registry keys: \"\n","            for key in value:\n","              s += key + \", \"\n","            behaviors.append(s)\n","          if key == \"regkey_written\":\n","            s += \"write registry keys: \"\n","            for key in value:\n","              s += key + \", \"\n","            behaviors.append(s)\n","          if key == \"dll_loaded\":\n","            s += \"load dlls: \"\n","            for dll in value:\n","              s += dll + \", \"\n","            behaviors.append(s)\n","    if  data[\"behavior\"][\"processtree\"] != []:\n","      s = \"Process tree of this malware: \\n\"\n","    for j in data[\"behavior\"][\"processtree\"]:\n","      s += str(j) + \"\\n\"\n","    behaviors.append(s)\n","    \n","    return behaviors"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:18.790155Z","iopub.status.busy":"2025-07-10T13:18:18.789473Z","iopub.status.idle":"2025-07-10T13:18:18.793667Z","shell.execute_reply":"2025-07-10T13:18:18.792962Z","shell.execute_reply.started":"2025-07-10T13:18:18.790131Z"},"trusted":true},"outputs":[],"source":["def process_dynamic(dynamic_data):\n","    print(\"Process dynamic\")\n","    result = {}\n","    result[\"behaviors\"] = extract_behavior(dynamic_data)\n","    result[\"signatures\"] = extract_signatures(dynamic_data)\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["##  Xử lý Static Analysis"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:20.814077Z","iopub.status.busy":"2025-07-10T13:18:20.813804Z","iopub.status.idle":"2025-07-10T13:18:20.818964Z","shell.execute_reply":"2025-07-10T13:18:20.818202Z","shell.execute_reply.started":"2025-07-10T13:18:20.814059Z"},"trusted":true},"outputs":[],"source":["def process_static(static_data):\n","    print(\"Process static\")\n","    result = {}\n","    file_path = list(static_data.keys())[0]\n","    for x, y in static_data[file_path][\"Plugins\"].items():\n","        result[x] = {}\n","        for a, b in y.items():\n","            if a == \"plugin_output\":\n","                \n","                for key, value in b.items():\n","                    result[x][key] = value\n","            if a == \"summary\":\n","                result[x][a] = b\n","    return result\n","                "]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2025-06-14T09:59:44.000205Z","iopub.status.busy":"2025-06-14T09:59:43.999958Z","iopub.status.idle":"2025-06-14T09:59:44.006514Z","shell.execute_reply":"2025-06-14T09:59:44.005842Z","shell.execute_reply.started":"2025-06-14T09:59:44.000187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'strings': {'Contains another PE executable': ['This program cannot be run in DOS mode.'], 'summary': 'Strings found in the binary may indicate undesirable behavior:'}, 'packer': {'info_0': 'Section .data is both writable and executable.', 'summary': 'The PE is possibly packed.'}, 'imports': {'Can access the registry': ['RegCloseKey', 'RegSetValueExA', 'RegOpenKeyExA'], 'Possibly launches other programs': ['CreateProcessA'], 'Functions related to the privilege level': ['OpenProcessToken', 'AdjustTokenPrivileges'], 'summary': 'The PE contains functions mostly used by malware.'}}\n"]}],"source":["import json\n","with open(\"/kaggle/input/static-analysis/21.json\") as f:\n","    raw_static = json.load(f)\n","file_path = list(raw_static.keys())[0]\n","print(process_static(raw_static))"]},{"cell_type":"markdown","metadata":{},"source":["## Query RAG"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:25.127613Z","iopub.status.busy":"2025-07-10T13:18:25.127099Z","iopub.status.idle":"2025-07-10T13:18:51.080170Z","shell.execute_reply":"2025-07-10T13:18:51.079307Z","shell.execute_reply.started":"2025-07-10T13:18:25.127575Z"},"trusted":true},"outputs":[],"source":["import torch\n","from sentence_transformers import SentenceTransformer\n","import chromadb\n","import shutil\n","\n","src_dir = \"/kaggle/input/chromadb\"\n","dst_dir = \"/kaggle/working/chromadb\"\n","\n","shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n","\n","PERSIST_DIR = \"/kaggle/working/chromadb\"\n","client = chromadb.PersistentClient(path=PERSIST_DIR)\n","collection = client.get_collection(name=\"ttp_behaviors\")\n","\n","def search_similar_behaviors(behavior_text, top_k=3):\n","    # Tạo embedding\n","    embedding = embedding_model.encode([behavior_text], convert_to_tensor=True, show_progress_bar=False).cpu().tolist()[0]\n","\n","    # Truy vấn từ vector database\n","    results = collection.query(\n","        query_embeddings=[embedding],\n","        n_results=top_k\n","    )\n","\n","    # Trích xuất kết quả\n","    matches = []\n","    for doc, meta, score in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n","        if score <= 0.4:\n","            matches.append({\n","                \"behavior\": doc,\n","                \"ttp\": meta[\"ttp\"]\n","                #\"score\": score\n","            })\n","\n","    return matches"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-06-15T08:38:32.450504Z","iopub.status.busy":"2025-06-15T08:38:32.449936Z","iopub.status.idle":"2025-06-15T08:38:32.474088Z","shell.execute_reply":"2025-06-15T08:38:32.473488Z","shell.execute_reply.started":"2025-06-15T08:38:32.450484Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'behavior': 'Reads information about supported languages \"3fc83a5369f55298d3513048f8a0ca78184b5cad8abf7d78f6728d62c19dcc70.exe\" (Path: \"HKLM\\\\SYSTEM\\\\CONTROLSET001\\\\CONTROL\\\\NLS\\\\CUSTOMLOCALE\"; Key: \"EMPTY\")', 'ttp': 'T1082', 'score': 0.27553480863571167}]\n"]}],"source":["print(search_similar_behaviors(\"Reads information about supported languages \\\"21.exe\\\" (Path: \\\"HKLM\\\\SYSTEM\\\\CONTROLSET001\\\\CONTROL\\\\NLS\\\\CUSTOMLOCALE\\\"; Key: \\\"EMPTY\\\")': ['Overall entropy of this PE file is high']}\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Web"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:18:55.721586Z","iopub.status.busy":"2025-07-10T13:18:55.720918Z","iopub.status.idle":"2025-07-10T13:18:55.743777Z","shell.execute_reply":"2025-07-10T13:18:55.742988Z","shell.execute_reply.started":"2025-07-10T13:18:55.721561Z"},"trusted":true},"outputs":[],"source":["\n","import json\n","import re\n","import copy\n","from collections import Counter\n","import pandas as pd\n","\n","def extract_first_json_array(text):\n","    matches = re.finditer(r'\\[\\s*{.*?}\\s*]', text, re.DOTALL)\n","    for match in matches:\n","        try:\n","            return json.loads(match.group())\n","        except json.JSONDecodeError:\n","            continue\n","    return None\n","\n","def split_into_chunks(data_list, chunk_size):\n","    return [data_list[i:i + chunk_size] for i in range(0, len(data_list), chunk_size)]\n","\n","\n","def infer_apt(observed_ttps):\n","    with open(\"/kaggle/input/ttp-map-apt/ttp_map_group.json\", \"r\") as f:\n","        ttp_to_groups = json.load(f)\n","    with open(\"/kaggle/input/group-map-name/group_map_name.json\", \"r\") as f:\n","        group_map_name = json.load(f)\n","    count = Counter()\n","    for ttp in observed_ttps:\n","        for group in ttp_to_groups.get(ttp, []):\n","            count[group] += 1\n","    if not count:\n","        return pd.DataFrame(columns=[\"Rank\", \"Group\", \"Score\"])\n","\n","    sorted_groups = sorted(count.items(), key=lambda x: (-x[1], x[0]))\n","    ranked_apt = []\n","    rank = 1\n","    prev_score = None\n","    for i, (group, score) in enumerate(sorted_groups, 1):\n","        if score != prev_score:\n","            rank = i\n","            prev_score = score\n","        ranked_apt.append({\"Rank\": rank, \"Group\": group_map_name[group], \"Score\": score})\n","    return pd.DataFrame(ranked_apt)\n","\n","\n","def generate_answer(static_json_file, dynamic_json_file):\n","    messages = []\n","    if static_json_file is None:\n","        return \"Static Analysis file is missing.\", None, None\n","    if dynamic_json_file is None:\n","        return \"Dynamic Analysis file is missing.\", None, None\n","    try:\n","        with open(static_json_file, \"r\") as f:\n","            static_data = json.load(f)\n","\n","    except Exception as e:\n","        messages.append(f\"Error loading Static JSON: {e}\")\n","        return \"\\n\".join(messages), None, None\n","\n","    try:\n","        with open(dynamic_json_file, \"r\") as f:\n","            dynamic_data = json.load(f)\n","\n","    except Exception as e:\n","        messages.append(f\"Error loading Dynamic JSON: {e}\")\n","        return \"\\n\".join(messages), None, None\n","\n","    ioc = ioc_extractors(dynamic_data, static_data)\n","    processed_static = process_static(static_data)\n","    processed_dynamic = process_dynamic(dynamic_data)\n","    tmp_processed_dynamic = copy.deepcopy(processed_dynamic)\n","    final_result = {}\n","    #print(\"Processed dynamic before filter: \", len(processed_dynamic[\"behaviors\"]), len(processed_dynamic[\"signatures\"]))\n","\n","    for key, value in tmp_processed_dynamic.items():\n","        if key == \"behaviors\":\n","            for b in range(len(value)):\n","                tmp = search_similar_behaviors(value[b])\n","                if tmp == []:\n","                    processed_dynamic[\"behaviors\"].remove(value[b])\n","        if key == \"signatures\":\n","            for b in range(len(value)):\n","                if value[b] == None: continue\n","                if \"ttp\" not in list(value[b].keys()):\n","                    tmp = []\n","                    if search_similar_behaviors(value[b][\"behavior\"]) != []:\n","                        tmp.append(search_similar_behaviors(value[b][\"behavior\"]))\n","                    for j in value[b][\"evidence\"]:\n","                        tmp_str = value[b][\"behavior\"] + j\n","                        if search_similar_behaviors(tmp_str) != []:\n","                            tmp.append(search_similar_behaviors(tmp_str))\n","                    if tmp == []:\n","                        processed_dynamic[\"signatures\"].remove(value[b])\n","\n","    #print(\"Processed dynamic after filter: \", len(processed_dynamic[\"behaviors\"]), len(processed_dynamic[\"signatures\"]))\n","    chunks = split_into_chunks(processed_dynamic[\"behaviors\"], chunk_size=10)\n","    #print(len(chunks))\n","    for num, chunk in enumerate(chunks):\n","        rag_result = []\n","        seen = set()\n","        for b in range(len(chunk)):\n","                tmp = search_similar_behaviors(chunk[b])\n","                for i in tmp:\n","                    if i[\"behavior\"] not in seen:\n","                        seen.add(i[\"behavior\"])\n","                        rag_result.append(i)\n","        prompt = f\"\"\"You are a cybersecurity analyst specialized in malware behavior analysis.\n","                Your task is to extract only valid MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) used by a suspicious file, based on multiple sources of analysis.\n","                Use the following input sources:\n","                static_analysis: Static characteristics such as suspicious strings, structurally detect packer presence, suspicious imports.\n","                dynamic_analysis: Runtime signatures behavior, some behaviors is assigned TTP by sandbox.\n","                iocs: Indicators of Compromise such as IPs, domains, urls and their information from Threat Intelligence.\n","                behavior_ttp_mapping: A list of pre-annotated behaviors with candidate TTPs from RAG. You can use this for reference only, but do not copy or cite evidence from this source.\n","                \n","                Reasoning Process (Chain-of-thought):\n","                For each observed behavior or pattern:\n","                - Determine whether it corresponds to a known MITRE ATT&CK technique.\n","                - Justify the mapping by explaining the intent or effect of the behavior.\n","                - Extract the correct MITRE ATT&CK technique ID and name.\n","                - Identify exact evidence strings from either static_analysis, dynamic_analysis, or iocs only.\n","                - Do not use any evidence from behavior_ttp_mapping for citations.\n","                - Only include mappings that are confident and valid. Avoid weak or speculative mappings.\n","\n","                Response only following output format (JSON) and do not add anything else:\n","                Output Format (JSON array):\n","                    \n","                \"technique_id\": \"Txxxx\",\n","                \"evidence\": [\n","                      \"Exact evidence string 1 from static/dynamic/IoC\",\n","                      \"Exact evidence string 2 from static/dynamic/IoC\"\n","                ],\n","                \"reasoning\": \"Explanation of why this evidence supports the mapping to the technique.\"\n","\n","                Note:\n","                – Merge multiple supporting behaviors into a single TTP mapping when appropriate  \n","                – Avoid duplicate TTP entries  \n","                – Only use official MITRE ATT&CK techniques\n","                \n","                Begin your analysis using the provided inputs:\n","                static_analysis:\n","                {processed_static}\n","                dynamic_analysis:\n","                {chunk}\n","                iocs:\n","                {ioc}\n","                behavior_ttp_mapping:\n","                {rag_result}\n","                \n","                Response:\n","                        \"\"\"\n","        ans = extract_first_json_array(get_gemini_response(prompt))\n","        #print(get_gemini_response(prompt))\n","        #print(f\"Response chunk {num}: \\n{ans}\")\n","        # Thêm vào mảng kết quả\n","        if ans != None:\n","            for ans_i in ans:\n","                if \"technique_id\" not in ans_i.keys():\n","                    print(ans_i)\n","                    continue\n","                if ans_i['technique_id'] not in final_result.keys():\n","                    final_result[ans_i['technique_id']] = []\n","                    tmp = {}\n","                    tmp[\"evidence\"] = ans_i['evidence']\n","                    tmp[\"reasoning\"] = ans_i['reasoning']\n","                    final_result[ans_i['technique_id']].append(tmp)\n","                else:\n","                    tmp = {}\n","                    tmp[\"evidence\"] = ans_i['evidence']\n","                    tmp[\"reasoning\"] = ans_i['reasoning']\n","                    final_result[ans_i['technique_id']].append(tmp)\n","    #print(\"Processed dynamic after filter: \", len(processed_dynamic[\"behaviors\"]), len(processed_dynamic[\"signatures\"]))\n","    rag_result = []\n","    seen1 = set()\n","    for items in processed_dynamic[\"signatures\"]:\n","        if items == None: continue\n","        if \"ttp\" not in list(items.keys()):\n","            tmp = []\n","            if search_similar_behaviors(items[\"behavior\"]) != []:\n","                tmp.append(search_similar_behaviors(items[\"behavior\"]))\n","            for j in items[\"evidence\"]:\n","                tmp_str = items[\"behavior\"] + j\n","                if search_similar_behaviors(tmp_str) != []:\n","                    tmp.append(search_similar_behaviors(tmp_str))\n","            for i in tmp:\n","                for k in i:\n","                    if k[\"behavior\"] not in seen1:\n","                        seen1.add(k[\"behavior\"])\n","                        rag_result.append(k)\n","    prompt = f\"\"\"You are a cybersecurity analyst specialized in malware behavior analysis.\n","                Your task is to extract only valid MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) used by a suspicious file, based on multiple sources of analysis.\n","                Use the following input sources:\n","                static_analysis: Static characteristics such as suspicious strings, structurally detect packer presence, suspicious imports.\n","                dynamic_analysis: Runtime signatures behavior, some behaviors is assigned TTP by sandbox.\n","                iocs: Indicators of Compromise such as IPs, domains, urls and their information from Threat Intelligence.\n","                behavior_ttp_mapping: A list of pre-annotated behaviors with candidate TTPs from RAG. You can use this for reference only, but do not copy or cite evidence from this source.\n","                \n","                Reasoning Process (Chain-of-thought):\n","                For each observed behavior or pattern:\n","                - Determine whether it corresponds to a known MITRE ATT&CK technique.\n","                - Justify the mapping by explaining the intent or effect of the behavior.\n","                - Extract the correct MITRE ATT&CK technique ID and name.\n","                - Identify exact evidence strings from either static_analysis, dynamic_analysis, or iocs only.\n","                - Do not use any evidence from behavior_ttp_mapping for citations.\n","                - Only include mappings that are confident and valid. Avoid weak or speculative mappings.\n","\n","                Response only following output format (JSON) and do not add anything else:\n","                Output Format (JSON array):\n","                    \n","                \"technique_id\": \"Txxxx\",\n","                \"evidence\": [\n","                      \"Exact evidence string 1 from static/dynamic/IoC\",\n","                      \"Exact evidence string 2 from static/dynamic/IoC\"\n","                ],\n","                \"reasoning\": \"Explanation of why this evidence supports the mapping to the technique.\"\n","\n","                Note:\n","                – Merge multiple supporting behaviors into a single TTP mapping when appropriate  \n","                – Avoid duplicate TTP entries  \n","                – Only use official MITRE ATT&CK techniques\n","                \n","                Begin your analysis using the provided inputs:\n","                static_analysis:\n","                {processed_static}\n","                dynamic_analysis:\n","                {processed_dynamic[\"signatures\"]}\n","                iocs:\n","                {ioc}\n","                behavior_ttp_mapping:\n","                {rag_result}\n","                \n","                Response:\n","                        \"\"\"\n","    ans = extract_first_json_array(get_gemini_response(prompt))\n","    #print(f\"Response signatures: \\n{ans}\")\n","     # Thêm vào mảng kết quả\n","    if ans != None:\n","        for ans_i in ans:\n","            if \"technique_id\" not in ans_i.keys():\n","                print(ans_i)\n","                continue\n","            if ans_i['technique_id'] not in final_result.keys():\n","                final_result[ans_i['technique_id']] = []\n","                tmp = {}\n","                tmp[\"evidence\"] = ans_i['evidence']\n","                tmp[\"reasoning\"] = ans_i['reasoning']\n","                final_result[ans_i['technique_id']].append(tmp)\n","            else:\n","                tmp = {}\n","                tmp[\"evidence\"] = ans_i['evidence']\n","                tmp[\"reasoning\"] = ans_i['reasoning']\n","                final_result[ans_i['technique_id']].append(tmp)\n","\n","    messages.append(f\"Extracted {len(final_result)} techniques.\")\n","    apt_df = infer_apt(list(final_result.keys()))\n","    return \"\\n\".join(messages), final_result, apt_df\n","    "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-07-10T13:50:00.757195Z","iopub.status.busy":"2025-07-10T13:50:00.756440Z","iopub.status.idle":"2025-07-10T13:50:02.461327Z","shell.execute_reply":"2025-07-10T13:50:02.460421Z","shell.execute_reply.started":"2025-07-10T13:50:00.757167Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["* Running on local URL:  http://127.0.0.1:7861\n","It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","* Running on public URL: https://0ee5454b06a4b130d4.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://0ee5454b06a4b130d4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":19,"metadata":{},"output_type":"execute_result"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["IOC Extractors\n","Process static\n","Process dynamic\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]}],"source":["import gradio as gr\n","\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## DACN\")\n","\n","    with gr.Row():\n","        static_input = gr.File(label=\"Upload Static Analysis (JSON)\")\n","        dynamic_input = gr.File(label=\"Upload Dynamic Analysis (JSON)\")\n","\n","    generate_btn = gr.Button(\"Generate Answer\")\n","\n","    output_log = gr.Textbox(label=\"Log / Status\")\n","    output_json = gr.JSON(label=\"Extracted TTPs\")\n","    output_df = gr.Dataframe(label=\"APT Groups List\")\n","\n","\n","    generate_btn.click(\n","        fn=generate_answer,\n","        inputs=[static_input, dynamic_input],\n","        outputs=[output_log, output_json, output_df]\n","    )\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{},"source":["##  Đánh giá tổng"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2025-07-06T09:23:24.133905Z","iopub.status.busy":"2025-07-06T09:23:24.133608Z","iopub.status.idle":"2025-07-06T09:23:24.140474Z","shell.execute_reply":"2025-07-06T09:23:24.139840Z","shell.execute_reply.started":"2025-07-06T09:23:24.133883Z"},"trusted":true},"outputs":[],"source":["import requests\n","def search_hashv2(hash):\n","    # URL của API\n","    url = \"https://hybrid-analysis.com/api/v2/search/hash\"\n","    \n","    headers = {\n","        \"accept\": \"application/json\",\n","        \"api-key\": \"s64d1p2o088bf64fcgpxlhu9fe9ef89a1mugw4bd2acb3098noaetygs2fb9cc9d\", \n","        \"Content-Type\": \"application/x-www-form-urlencoded\",\n","    }\n","\n","    data = {\n","        \"hash\": hash, \n","    }\n","    response = requests.post(url, headers=headers, data=data)\n","    if response.status_code == 200:\n","        result = response.json()\n","        filtered_result = {}\n","        for i in range(len(result)):\n","          if result[i]['signatures'] != []:\n","            for j in range(len(result[i]['signatures'])):\n","                if result[i]['signatures'][j]['attck_id'] != None:\n","                    if result[i]['signatures'][j]['attck_id'] not in list(filtered_result.keys()):\n","                        filtered_result[result[i]['signatures'][j]['attck_id']] = []\n","                    for k in result[i]['signatures'][j]['description'].split(\"\\n\"):\n","                        filtered_result[result[i]['signatures'][j]['attck_id']].append(k)\n","        return filtered_result if filtered_result != [] else None\n","    else:\n","        print(f\"hash - Request failed with status code {response.status_code}\")\n","        return None"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-07-06T09:23:20.952642Z","iopub.status.busy":"2025-07-06T09:23:20.951983Z","iopub.status.idle":"2025-07-06T09:23:20.974757Z","shell.execute_reply":"2025-07-06T09:23:20.973881Z","shell.execute_reply.started":"2025-07-06T09:23:20.952616Z"},"trusted":true},"outputs":[],"source":["import copy\n","\n","import re\n","import json\n","# Hàm trích xuất câu trả lời từ LLM\n","def extract_first_json_array(text):\n","    # Tìm tất cả các đoạn giống mảng JSON\n","    matches = re.finditer(r'\\[\\s*{.*?}\\s*]', text, re.DOTALL)\n","    for match in matches:\n","        try:\n","            return json.loads(match.group())\n","        except json.JSONDecodeError:\n","            continue\n","    return None\n","\n","from collections import Counter\n","\n","def infer_apt(observed_ttps):\n","    with open(\"/kaggle/input/ttp-map-apt/ttp_map_group.json\", \"r\") as f:\n","        ttp_to_groups = json.load(f)\n","    count = Counter()\n","    for ttp in observed_ttps:\n","        for group in ttp_to_groups.get(ttp, []):\n","            count[group] += 1\n","\n","    if not count:\n","        return []\n","\n","    # Sắp xếp giảm dần theo tần suất\n","    sorted_groups = sorted(count.items(), key=lambda x: (-x[1], x[0]))\n","    \n","    # Tạo list kết quả\n","    ranked_apt = []\n","    \n","    # Biến lưu thông tin rank\n","    rank = 1\n","    prev_score = None\n","    \n","    for i, (group, score) in enumerate(sorted_groups, 1):\n","        if score != prev_score:\n","            rank = i\n","            prev_score = score\n","  \n","        ranked_apt.append((group, score, rank))\n","    \n","    return ranked_apt\n","\n","# Biến trung gian\n","cti_texts, index = [], None\n","static_texts, dynamic_texts, malware_texts, report_data = [], [], [], []\n","\n","def split_into_chunks(data_list, chunk_size):\n","    return [data_list[i:i+chunk_size] for i in range(0, len(data_list), chunk_size)]\n","\n","# Xử lý báo cáo JSON\n","def on_process_report_clicked(static_path, dynamic_path):\n","    global static_texts\n","    global dynamic_texts\n","    with open(static_path, 'r', encoding='utf-8') as f:\n","        static_texts = json.load(f)\n","    with open(dynamic_path, 'r', encoding='utf-8') as f:\n","        dynamic_texts = json.load(f)\n","\n","\n","\n","def on_generate_clicked():    \n","    final_result = {}\n","    ioc = ioc_extractors(dynamic_texts, static_texts)\n","    processed_static = process_static(static_texts)\n","    processed_dynamic = process_dynamic(dynamic_texts)\n","    tmp_processed_dynamic = copy.deepcopy(processed_dynamic)\n","    \n","    print(\"Processed dynamic before filter: \", len(processed_dynamic[\"behaviors\"]), len(processed_dynamic[\"signatures\"]))\n","    # rag_result = []\n","    # seen = set()\n","    for key, value in tmp_processed_dynamic.items():\n","        if key == \"behaviors\":\n","            for b in range(len(value)):\n","                tmp = search_similar_behaviors(value[b])\n","                if tmp == []:\n","                    processed_dynamic[\"behaviors\"].remove(value[b])\n","                # else:\n","                #     for i in tmp:\n","                #         if i[\"behavior\"] not in seen:\n","                #             seen.add(i[\"behavior\"])\n","                #             rag_result.append(i)\n","        if key == \"signatures\":\n","            for b in range(len(value)):\n","                if value[b] == None: continue\n","                if \"ttp\" not in list(value[b].keys()):\n","                    tmp = []\n","                    if search_similar_behaviors(value[b][\"behavior\"]) != []:\n","                        tmp.append(search_similar_behaviors(value[b][\"behavior\"]))\n","                    for j in value[b][\"evidence\"]:\n","                        tmp_str = value[b][\"behavior\"] + j\n","                        if search_similar_behaviors(tmp_str) != []:\n","                            tmp.append(search_similar_behaviors(tmp_str))\n","                    if tmp == []:\n","                        processed_dynamic[\"signatures\"].remove(value[b])\n","                    # else:\n","                    #     for item in tmp:\n","                    #         for k in item:\n","                    #             if k[\"behavior\"] not in seen:\n","                    #                 seen.add(k[\"behavior\"])\n","                    #                 rag_result.append(k)\n","    print(\"Processed dynamic after filter: \", len(processed_dynamic[\"behaviors\"]), len(processed_dynamic[\"signatures\"]))\n","    chunks = split_into_chunks(processed_dynamic[\"behaviors\"], chunk_size=10)\n","    print(len(chunks))\n","    # Bỏ những mẫu có số lượng chunk lớn hơn 15 vì giới hạn api\n","    if len(chunks) > 15: return None\n","    for num, chunk in enumerate(chunks):\n","        rag_result = []\n","        seen = set()\n","        for b in range(len(chunk)):\n","                tmp = search_similar_behaviors(chunk[b])\n","                for i in tmp:\n","                    if i[\"behavior\"] not in seen:\n","                        seen.add(i[\"behavior\"])\n","                        rag_result.append(i)\n","        prompt = f\"\"\"You are a cybersecurity analyst specialized in malware behavior analysis.\n","                Your task is to extract only valid MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) used by a suspicious file, based on multiple sources of analysis.\n","                Use the following input sources:\n","                static_analysis: Static characteristics such as suspicious strings, structurally detect packer presence, suspicious imports.\n","                dynamic_analysis: Runtime signatures behavior, some behaviors is assigned TTP by sandbox.\n","                iocs: Indicators of Compromise such as IPs, domains, urls and their information from Threat Intelligence.\n","                behavior_ttp_mapping: A list of pre-annotated behaviors with candidate TTPs from RAG. You can use this for reference only, but do not copy or cite evidence from this source.\n","                \n","                Reasoning Process (Chain-of-thought):\n","                For each observed behavior or pattern:\n","                - Determine whether it corresponds to a known MITRE ATT&CK technique.\n","                - Justify the mapping by explaining the intent or effect of the behavior.\n","                - Extract the correct MITRE ATT&CK technique ID and name.\n","                - Identify exact evidence strings from either static_analysis, dynamic_analysis, or iocs only.\n","                - Do not use any evidence from behavior_ttp_mapping for citations.\n","                - Only include mappings that are confident and valid. Avoid weak or speculative mappings.\n","                \n","                Response only following output format (JSON) and do not add anything else:\n","                Output Format (JSON array):\n","                    \n","                \"technique_id\": \"Txxxx\",\n","                \"evidence\": [\n","                      \"Exact evidence string 1 from static/dynamic/IoC\",\n","                      \"Exact evidence string 2 from static/dynamic/IoC\"\n","                ],\n","                \"reasoning\": \"Explanation of why this evidence supports the mapping to the technique.\"\n","                \n","                Note:\n","                – Merge multiple supporting behaviors into a single TTP mapping when appropriate  \n","                – Avoid duplicate TTP entries  \n","                – Only use official MITRE ATT&CK techniques\n","                \n","                Begin your analysis using the provided inputs:\n","                static_analysis: \n","                {processed_static}\n","                dynamic_analysis:\n","                {chunk}\n","                iocs:\n","                {ioc}\n","                behavior_ttp_mapping:\n","                {rag_result}\n","                \n","                Response:\n","                        \"\"\"\n","        ans = extract_first_json_array(get_gemini_response(prompt))\n","        #print(f\"Response chunk {num}: \\n{get_gemini_response(prompt)}\")\n","        if ans != None:\n","            for ans_i in ans:\n","                if \"technique_id\" not in ans_i.keys():\n","                    print(ans_i)\n","                    continue\n","                if ans_i['technique_id'] not in final_result.keys():\n","                    final_result[ans_i['technique_id']] = ans_i['evidence']\n","                else:\n","                    for e in ans_i['evidence']:\n","                        final_result[ans_i['technique_id']].append(e)\n","    #print(\"Processed dynamic after filter: \", len(processed_dynamic[\"behaviors\"]), len(processed_dynamic[\"signatures\"]))\n","    rag_result = []\n","    seen1 = set()\n","    for items in processed_dynamic[\"signatures\"]:\n","        if items == None: continue\n","        if \"ttp\" not in list(items.keys()):\n","            tmp = []\n","            if search_similar_behaviors(items[\"behavior\"]) != []:\n","                tmp.append(search_similar_behaviors(items[\"behavior\"]))\n","            for j in items[\"evidence\"]:\n","                tmp_str = items[\"behavior\"] + j\n","                if search_similar_behaviors(tmp_str) != []:\n","                    tmp.append(search_similar_behaviors(tmp_str))\n","            for i in tmp:\n","                for k in i:\n","                    if k[\"behavior\"] not in seen1:\n","                        seen1.add(k[\"behavior\"])\n","                        rag_result.append(k)\n","    prompt = f\"\"\"You are a cybersecurity analyst specialized in malware behavior analysis.\n","                Your task is to extract only valid MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) used by a suspicious file, based on multiple sources of analysis.\n","                Use the following input sources:\n","                static_analysis: Static characteristics such as suspicious strings, structurally detect packer presence, suspicious imports.\n","                dynamic_analysis: Runtime signatures behavior, some behaviors is assigned TTP by sandbox.\n","                iocs: Indicators of Compromise such as IPs, domains, urls and their information from Threat Intelligence.\n","                behavior_ttp_mapping: A list of pre-annotated behaviors with candidate TTPs from RAG. You can use this for reference only, but do not copy or cite evidence from this source.\n","                \n","                Reasoning Process (Chain-of-thought):\n","                For each observed behavior or pattern:\n","                - Determine whether it corresponds to a known MITRE ATT&CK technique.\n","                - Justify the mapping by explaining the intent or effect of the behavior.\n","                - Extract the correct MITRE ATT&CK technique ID and name.\n","                - Identify exact evidence strings from either static_analysis, dynamic_analysis, or iocs only.\n","                - Do not use any evidence from behavior_ttp_mapping for citations.\n","                - Only include mappings that are confident and valid. Avoid weak or speculative mappings.\n","\n","                Response only following output format (JSON) and do not add anything else:\n","                Output Format (JSON array):\n","                    \n","                \"technique_id\": \"Txxxx\",\n","                \"evidence\": [\n","                      \"Exact evidence string 1 from static/dynamic/IoC\",\n","                      \"Exact evidence string 2 from static/dynamic/IoC\"\n","                ],\n","                \"reasoning\": \"Explanation of why this evidence supports the mapping to the technique.\"\n","                \n","                Note:\n","                – Merge multiple supporting behaviors into a single TTP mapping when appropriate  \n","                – Avoid duplicate TTP entries  \n","                – Only use official MITRE ATT&CK techniques\n","                \n","                Begin your analysis using the provided inputs:\n","                static_analysis:\n","                {processed_static}\n","                dynamic_analysis:\n","                {processed_dynamic[\"signatures\"]}\n","                iocs:\n","                {ioc}\n","                behavior_ttp_mapping:\n","                {rag_result}\n","\n","                Response:\n","                        \"\"\"\n","    ans = extract_first_json_array(get_gemini_response(prompt))\n","    #print(f\"Response signatures: \\n{get_gemini_response(prompt)}\")\n","    if ans != None:\n","        for ans_i in ans:\n","            if ans_i['technique_id'] not in final_result.keys():\n","                final_result[ans_i['technique_id']] = ans_i['evidence']\n","            else:\n","                for e in ans_i['evidence']:\n","                    final_result[ans_i['technique_id']].append(e)\n","    return final_result\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2025-07-06T09:23:33.020804Z","iopub.status.busy":"2025-07-06T09:23:33.020157Z","iopub.status.idle":"2025-07-06T09:23:33.024944Z","shell.execute_reply":"2025-07-06T09:23:33.024155Z","shell.execute_reply.started":"2025-07-06T09:23:33.020779Z"},"trusted":true},"outputs":[],"source":["def calc_rr(rank_list, ground_truth):\n","    rank = None\n","    for item in rank_list:\n","        group_id, score, r = item\n","        if group_id == ground_truth:\n","            rank = r\n","            print(group_id, score, r)\n","            break\n","    \n","    if rank is not None:\n","        return 1 / rank\n","    else:\n","        return 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import time\n","import json\n","\n","REPORT_DIR = \"/kaggle/input/dataset-apt-malware/Winnti/\"\n","with open('/kaggle/input/eval-file/eval_file.json', 'r') as f:\n","    eval_file = json.load(f)\n","cnt = 0\n","CR = 0\n","cntv2 = 0\n","\n","for file in os.listdir(REPORT_DIR):\n","    if cnt == 5: break\n","    if file in eval_file: continue\n","    print(file)\n","    eval_file.append(file)\n","    with open('/kaggle/working/eval_file.json', 'w') as f:\n","        json.dump(eval_file, f, indent=4)\n","   \n","    cnt += 1\n","    report = REPORT_DIR + file\n","    ground = search_hashv2(file)\n","    while ground == None:\n","        ground = search_hashv2(file)\n","        cntv2 += 1\n","        if cntv2 == 10:\n","            break\n","    cntv2 = 0\n","    if ground == None:\n","        cnt -= 1\n","        continue\n","    static_path = report + \"/\" + file + \".json\"\n","    dynamic_path = report + \"/\" + \"report.json\"\n","    start_time = time.time()\n","    on_process_report_clicked(static_path, dynamic_path)\n","    final_result = on_generate_clicked()\n","    if final_result == None: \n","        cnt-=1\n","        end_time = time.time()\n","        continue\n","    #print(final_result)\n","    file_result = \"/kaggle/working/3-\" + file + \".json\"\n","    with open(file_result, 'w') as f:\n","        json.dump(final_result, f, indent=4)\n","    set_g = set(list(final_result.keys()))\n","    set_r = set(list(ground.keys())) \n","    print(len(set_g & set_r))\n","    print(len(set_g))\n","    print(len(set_r))\n","    CR += len(set_g & set_r) / len(set_r)\n","    print(f\"{file} - CR_sum = {CR:.4f} - CR_cur = {(len(set_g & set_r) / len(set_r)):.4f}\")\n","    print(\"Reciprocal Rank (RR):\", calc_rr(infer_apt(list(final_result.keys())), \"G0016\"), \"Ground Truth:\", os.path.basename(os.path.normpath(REPORT_DIR)))\n","    end_time = time.time()\n","\n","    elapsed_time = end_time - start_time\n","    hours = int(elapsed_time // 3600)\n","    minutes = int((elapsed_time % 3600) // 60)\n","    seconds = elapsed_time % 60\n","    \n","    print(f\"Run time: {hours} h {minutes} m {seconds:.2f} s - {elapsed_time:.2f}s\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Đánh giá chỉ LLM"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2025-07-07T07:29:55.355180Z","iopub.status.busy":"2025-07-07T07:29:55.354555Z","iopub.status.idle":"2025-07-07T07:29:55.361586Z","shell.execute_reply":"2025-07-07T07:29:55.360885Z","shell.execute_reply.started":"2025-07-07T07:29:55.355154Z"},"trusted":true},"outputs":[],"source":["import requests\n","def search_hashv2(hash):\n","    # URL của API\n","    url = \"https://hybrid-analysis.com/api/v2/search/hash\"\n","    \n","    headers = {\n","        \"accept\": \"application/json\",\n","        \"api-key\": \"s64d1p2o088bf64fcgpxlhu9fe9ef89a1mugw4bd2acb3098noaetygs2fb9cc9d\", \n","        \"Content-Type\": \"application/x-www-form-urlencoded\",\n","    }\n","\n","    data = {\n","        \"hash\": hash,\n","    }\n","    response = requests.post(url, headers=headers, data=data)\n","    if response.status_code == 200:\n","        result = response.json()\n","        filtered_result = {}\n","        for i in range(len(result)):\n","          if result[i]['signatures'] != []:\n","            for j in range(len(result[i]['signatures'])):\n","                if result[i]['signatures'][j]['attck_id'] != None:\n","                    if result[i]['signatures'][j]['attck_id'] not in list(filtered_result.keys()):\n","                        filtered_result[result[i]['signatures'][j]['attck_id']] = []\n","                    for k in result[i]['signatures'][j]['description'].split(\"\\n\"):\n","                        filtered_result[result[i]['signatures'][j]['attck_id']].append(k)\n","        return filtered_result if filtered_result != [] else None\n","    else:\n","        print(f\"hash - Request failed with status code {response.status_code}\")\n","        return None"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-07-07T07:26:52.151632Z","iopub.status.busy":"2025-07-07T07:26:52.151139Z","iopub.status.idle":"2025-07-07T07:26:52.166643Z","shell.execute_reply":"2025-07-07T07:26:52.165903Z","shell.execute_reply.started":"2025-07-07T07:26:52.151608Z"},"trusted":true},"outputs":[],"source":["import copy\n","\n","import re\n","import json\n","\n","# Hàm trích xuất câu trả lời từ LLM\n","def extract_first_json_array(text):\n","    # Tìm tất cả các đoạn giống mảng JSON\n","    matches = re.finditer(r'\\[\\s*{.*?}\\s*]', text, re.DOTALL)\n","    for match in matches:\n","        try:\n","            return json.loads(match.group())\n","        except json.JSONDecodeError:\n","            continue\n","    return None\n","\n","from collections import Counter\n","\n","def infer_apt(observed_ttps):\n","    with open(\"/kaggle/input/ttp-map-apt/ttp_map_group.json\", \"r\") as f:\n","        ttp_to_groups = json.load(f)\n","    count = Counter()\n","    for ttp in observed_ttps:\n","        for group in ttp_to_groups.get(ttp, []):\n","            count[group] += 1\n","\n","    if not count:\n","        return []\n","\n","    # Sắp xếp giảm dần theo tần suất\n","    sorted_groups = sorted(count.items(), key=lambda x: (-x[1], x[0]))\n","    \n","    # Tạo list kết quả\n","    ranked_apt = []\n","    \n","    # Biến lưu thông tin rank\n","    rank = 1\n","    prev_score = None\n","    \n","    for i, (group, score) in enumerate(sorted_groups, 1):\n","        if score != prev_score:\n","            rank = i\n","            prev_score = score\n","        ranked_apt.append((group, score, rank))\n","\n","    return ranked_apt\n","\n","# Biến trung gian\n","cti_texts, index = [], None\n","static_texts, dynamic_texts, malware_texts, report_data = [], [], [], []\n","\n","def split_into_chunks(data_list, chunk_size):\n","    return [data_list[i:i+chunk_size] for i in range(0, len(data_list), chunk_size)]\n","\n","# Xử lý báo cáo JSON\n","def on_process_report_clicked(static_path, dynamic_path):\n","    global static_texts\n","    global dynamic_texts\n","    with open(static_path, 'r', encoding='utf-8') as f:\n","        static_texts = json.load(f)\n","    with open(dynamic_path, 'r', encoding='utf-8') as f:\n","        dynamic_texts = json.load(f)\n","\n","\n","\n","def on_generate_clicked():    \n","    final_result = {}\n","    ioc = ioc_extractors(dynamic_texts, static_texts)\n","    processed_static = process_static(static_texts)\n","    processed_dynamic = process_dynamic(dynamic_texts)\n","    tmp_processed_dynamic = copy.deepcopy(processed_dynamic)\n","    \n","    chunks = split_into_chunks(processed_dynamic[\"behaviors\"], chunk_size=14)\n","    print(len(chunks))\n","    if len(chunks) > 15: return None\n","    for num, chunk in enumerate(chunks):\n","        rag_result = []\n","        seen = set()\n","\n","        prompt = f\"\"\"You are a cybersecurity analyst specialized in malware behavior analysis.\n","                Your task is to extract only valid MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) used by a suspicious file, based on multiple sources of analysis.\n","                Use the following input sources:\n","                static_analysis: Static characteristics such as suspicious strings, structurally detect packer presence, suspicious imports.\n","                dynamic_analysis: Runtime signatures behavior, some behaviors is assigned TTP by sandbox.\n","                \n","                Reasoning Process (Chain-of-thought):\n","                For each observed behavior or pattern:\n","                - Determine whether it corresponds to a known MITRE ATT&CK technique.\n","                - Justify the mapping by explaining the intent or effect of the behavior.\n","                - Extract the correct MITRE ATT&CK technique ID and name.\n","                - Identify exact evidence strings from either static_analysis, dynamic_analysis, or iocs only.\n","                - Do not use any evidence from behavior_ttp_mapping for citations.\n","                - Only include mappings that are confident and valid. Avoid weak or speculative mappings.\n","                \n","                Response only following output format (JSON) and do not add anything else:\n","                Output Format (JSON array):\n","                    \n","                \"technique_id\": \"Txxxx\",\n","                \"evidence\": [\n","                      \"Exact evidence string 1 from static/dynamic/IoC\",\n","                      \"Exact evidence string 2 from static/dynamic/IoC\"\n","                ],\n","                \"reasoning\": \"Explanation of why this evidence supports the mapping to the technique.\"\n","\n","                Note:\n","                – Merge multiple supporting behaviors into a single TTP mapping when appropriate  \n","                – Avoid duplicate TTP entries  \n","                – Only use official MITRE ATT&CK techniques\n","                \n","                Begin your analysis using the provided inputs:\n","                static_analysis:\n","                {processed_static}\n","                \n","                dynamic_analysis:\n","                {chunk}\n","                \n","                Response\n","                        \"\"\"\n","        ans = extract_first_json_array(get_gemini_response(prompt))\n","        if ans != None:\n","            for ans_i in ans:\n","                if \"technique_id\" not in ans_i.keys():\n","                    print(ans_i)\n","                    continue\n","                if ans_i['technique_id'] not in final_result.keys():\n","                    final_result[ans_i['technique_id']] = ans_i['evidence']\n","                else:\n","                    for e in ans_i['evidence']:\n","                        final_result[ans_i['technique_id']].append(e)\n","\n","    prompt = f\"\"\"You are a cybersecurity analyst specialized in malware behavior analysis and MITRE ATT&CK mapping.\n","                Your task is to extract only valid MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) used by a suspicious file, based on multiple sources of analysis.\n","                Use the following input sources:\n","                static_analysis: Static characteristics such as suspicious strings, structurally detect packer presence, suspicious imports.\n","                dynamic_analysis: Runtime signatures behavior, some behaviors is assigned TTP by sandbox.\n","\n","                Reasoning Process (Chain-of-thought):\n","                For each observed behavior or pattern:\n","                - Determine whether it corresponds to a known MITRE ATT&CK technique.\n","                - Justify the mapping by explaining the intent or effect of the behavior.\n","                - Extract the correct MITRE ATT&CK technique ID and name.\n","                - Identify exact evidence strings from either static_analysis, dynamic_analysis, or iocs only.\n","                - Do not use any evidence from behavior_ttp_mapping for citations.\n","                - Only include mappings that are confident and valid. Avoid weak or speculative mappings.\n","\n","                Response only following output format (JSON) and do not add anything else:\n","                Output Format (JSON array):\n","                    \n","                \"technique_id\": \"Txxxx\",\n","                \"evidence\": [\n","                      \"Exact evidence string 1 from static/dynamic/IoC\",\n","                      \"Exact evidence string 2 from static/dynamic/IoC\"\n","                ],\n","                \"reasoning\": \"Explanation of why this evidence supports the mapping to the technique.\"\n","                \n","                Note:\n","                – Merge multiple supporting behaviors into a single TTP mapping when appropriate  \n","                – Avoid duplicate TTP entries  \n","                – Only use official MITRE ATT&CK techniques\n","                \n","                Begin your analysis using the provided inputs:\n","                static_analysis:\n","                {processed_static}\n","                dynamic_analysis:\n","                {processed_dynamic[\"signatures\"]}\n","\n","                Response:\n","                        \"\"\"\n","    ans = extract_first_json_array(get_gemini_response(prompt))\n","    #print(f\"Response signatures: \\n{get_gemini_response(prompt)}\")\n","    if ans != None:\n","        for ans_i in ans:\n","            if ans_i['technique_id'] not in final_result.keys():\n","                final_result[ans_i['technique_id']] = ans_i['evidence']\n","            else:\n","                for e in ans_i['evidence']:\n","                    final_result[ans_i['technique_id']].append(e)\n","    return final_result\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-07-07T07:27:07.011973Z","iopub.status.busy":"2025-07-07T07:27:07.011386Z","iopub.status.idle":"2025-07-07T07:27:07.016220Z","shell.execute_reply":"2025-07-07T07:27:07.015343Z","shell.execute_reply.started":"2025-07-07T07:27:07.011947Z"},"trusted":true},"outputs":[],"source":["def calc_rr(rank_list, ground_truth):\n","    rank = None\n","    for item in rank_list:\n","        group_id, score, r = item\n","        if group_id == ground_truth:\n","            rank = r\n","            print(group_id, score, r)\n","            break\n","    \n","    if rank is not None:\n","        return 1 / rank\n","    else:\n","        return 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import time\n","import json\n","\n","REPORT_DIR = \"/kaggle/input/dataset-apt-malware/Winnti/\"\n","with open('/kaggle/input/eval-file/eval_file.json', 'r') as f:\n","    eval_file = json.load(f)\n","cnt = 0\n","CR = 0\n","cntv2 = 0\n","for file in os.listdir(REPORT_DIR):\n","    if cnt == 9: break\n","    if file in eval_file: continue\n","    print(file)\n","    eval_file.append(file)\n","    with open('/kaggle/working/eval_file.json', 'w') as f:\n","        json.dump(eval_file, f, indent=4)\n","    cnt += 1\n","    report = REPORT_DIR + file\n","    ground = search_hashv2(file)\n","    while ground == None:\n","        ground = search_hashv2(file)\n","        cntv2 += 1\n","        if cntv2 == 10:\n","            break\n","    cntv2 = 0\n","    if ground == None:\n","        cnt -= 1\n","        continue\n","    static_path = report + \"/\" + file + \".json\"\n","    dynamic_path = report + \"/\" + \"report.json\"\n","    start_time = time.time()\n","    on_process_report_clicked(static_path, dynamic_path)\n","    final_result = on_generate_clicked()\n","    if final_result == None: \n","        cnt-=1\n","        end_time = time.time()\n","        continue\n","    #print(final_result)\n","    file_result = \"/kaggle/working/\" + file + \".json\"\n","    with open(file_result, 'w') as f:\n","        json.dump(final_result, f, indent=4)\n","    set_g = set(list(final_result.keys()))\n","    set_r = set(list(ground.keys())) \n","    print(len(set_g & set_r))\n","    print(len(set_g))\n","    print(len(set_r))\n","    CR += len(set_g & set_r) / len(set_r)\n","    print(f\"{file} - CR_sum = {CR:.4f} - CR_cur = {(len(set_g & set_r) / len(set_r)):.4f}\")\n","    print(\"Reciprocal Rank (RR):\", calc_rr(infer_apt(list(final_result.keys())), \"G0044\"), \"Ground Truth:\", os.path.basename(os.path.normpath(REPORT_DIR)))\n","    end_time = time.time()\n","\n","    elapsed_time = end_time - start_time\n","    hours = int(elapsed_time // 3600)\n","    minutes = int((elapsed_time % 3600) // 60)\n","    seconds = elapsed_time % 60\n","    \n","    print(f\"Run time: {hours} h {minutes} m {seconds:.2f} s - {elapsed_time:.2f}s\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":7655705,"sourceId":12155829,"sourceType":"datasetVersion"},{"datasetId":7751759,"sourceId":12298825,"sourceType":"datasetVersion"},{"datasetId":7771878,"sourceId":12341675,"sourceType":"datasetVersion"},{"datasetId":7783094,"sourceId":12390330,"sourceType":"datasetVersion"},{"datasetId":7839417,"sourceId":12428619,"sourceType":"datasetVersion"}],"dockerImageVersionId":31041,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
